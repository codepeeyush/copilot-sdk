# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Models - use small models for quick testing:
# OLLAMA_MODEL=qwen2.5:1.5b   # ~986MB - smallest with tool support (recommended)
# OLLAMA_MODEL=qwen2:0.5b     # ~352MB - basic chat only (no tools)
OLLAMA_MODEL=llama3.1         # ~4.7GB - full featured

# Vision model (for vision demo)
OLLAMA_VISION_MODEL=llava

# Server Configuration
PORT=3002
