---
title: Deployment
description: Deploy your Copilot backend to any platform
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';

Your Copilot backend uses standard Web APIs (`fetch`, `Response`, `ReadableStream`), so the **same code runs everywhere**.

<Callout type="info">
**Flexible by Design** — The SDK works with any platform that supports Web APIs. The examples below are just starting points. Feel free to adapt them to your stack — whether it's Fastify, Koa, Bun, or your own custom server.
</Callout>

---

## Vercel

Deploy to Vercel with Next.js. Supports both Serverless and Edge runtimes.

<Tabs items={['Serverless (Default)', 'Edge Runtime']}>
  <Tab value="Serverless (Default)">
    ```ts title="app/api/chat/route.ts"
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });

    export async function POST(req: Request) {
      const body = await req.json();
      return runtime.stream(body).toResponse();
    }
    ```
  </Tab>
  <Tab value="Edge Runtime">
    Edge functions have faster cold starts (~25ms vs ~250ms) and run closer to users.

    ```ts title="app/api/chat/route.ts"
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    export const runtime = 'edge'; // Enable Edge Runtime

    const rt = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });

    export async function POST(req: Request) {
      const body = await req.json();
      return rt.stream(body).toResponse();
    }
    ```

    <Callout type="info">
    Edge functions have a 30-second execution limit. For long-running agent loops, use Serverless.
    </Callout>
  </Tab>
</Tabs>

### Non-Streaming

```ts title="app/api/chat/route.ts"
export async function POST(req: Request) {
  const body = await req.json();
  const result = await runtime.chat(body);
  return Response.json(result);
}
```

### Deploy

```bash
npm i -g vercel
vercel
```

Set your environment variable in the Vercel dashboard or CLI:

```bash
vercel env add OPENAI_API_KEY
```

---

## Cloudflare Workers

Deploy to Cloudflare's edge network. Runs in 300+ locations worldwide.

<Callout>
**Full Example Available** — See the complete [Cloudflare Workers Demo](https://github.com/YourGPT/copilot-sdk/tree/main/examples/cloudflare-workers-demo) with streaming and non-streaming endpoints.
</Callout>

<Tabs items={['Streaming', 'Non-Streaming']}>
  <Tab value="Streaming">
    ```ts title="src/index.ts"
    import { createRuntime, createHonoApp } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    export interface Env {
      OPENAI_API_KEY: string;
    }

    export default {
      async fetch(request: Request, env: Env): Promise<Response> {
        const runtime = createRuntime({
          provider: createOpenAI({ apiKey: env.OPENAI_API_KEY }),
          model: 'gpt-4o',
          systemPrompt: 'You are a helpful assistant.',
        });

        return createHonoApp(runtime).fetch(request, env);
      },
    };
    ```
  </Tab>
  <Tab value="Non-Streaming">
    ```ts title="src/index.ts"
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    export interface Env {
      OPENAI_API_KEY: string;
    }

    export default {
      async fetch(request: Request, env: Env): Promise<Response> {
        const runtime = createRuntime({
          provider: createOpenAI({ apiKey: env.OPENAI_API_KEY }),
          model: 'gpt-4o',
          systemPrompt: 'You are a helpful assistant.',
        });

        if (request.method !== 'POST') {
          return new Response('Method not allowed', { status: 405 });
        }

        const body = await request.json();
        const result = await runtime.chat(body);
        return Response.json(result);
      },
    };
    ```
  </Tab>
</Tabs>

### Configuration

```toml title="wrangler.toml"
name = "my-copilot-api"
main = "src/index.ts"
compatibility_date = "2024-01-01"
compatibility_flags = ["nodejs_compat"]
```

### Deploy

```bash
npm i -g wrangler

# Add your API key as a secret
wrangler secret put OPENAI_API_KEY

# Deploy
wrangler deploy
```

<Callout>
Your API will be available at `https://my-copilot-api.<your-subdomain>.workers.dev`
</Callout>

---

## More Platforms

<Callout type="warn">
**Coming Soon** — Detailed guides for these platforms are in progress.
</Callout>

The SDK works out of the box with these platforms. Use the patterns from Vercel/Cloudflare as a reference:

| Platform | Runtime | Key Method |
|----------|---------|------------|
| **Deno Deploy** | Edge | `Deno.serve(createHonoApp(runtime).fetch)` |
| **AWS Lambda** | Serverless | `runtime.chat(body)` with Function URL |
| **Express / Node.js** | Node.js | `runtime.stream(body).pipeToResponse(res)` |
| **Bun** | Bun | `export default { fetch: createHonoApp(runtime).fetch }` |
| **Docker** | Any | Use `@hono/node-server` with `createHonoApp` |
| **Railway / Render / Fly.io** | Node.js | Same as Express |

<Callout type="info">
**Try it yourself!** The SDK uses standard Web APIs, so if your platform supports `Request`/`Response`, it will work. Check the [Server Setup](/docs/server) page for all available methods like `.toResponse()`, `.pipeToResponse()`, and `.chat()`.
</Callout>

---

## Connect Frontend

Point your Copilot SDK frontend to your deployed API:

```tsx title="app/providers.tsx"
'use client';

import { CopilotProvider } from '@yourgpt/copilot-sdk/react';

export function Providers({ children }: { children: React.ReactNode }) {
  return (
    <CopilotProvider
      runtimeUrl="https://your-api.example.com/api/chat"
      // For non-streaming:
      // streaming={false}
    >
      {children}
    </CopilotProvider>
  );
}
```

| Mode | Server Method | CopilotProvider |
|------|---------------|-----------------|
| Streaming | `.stream(body).toResponse()` | `streaming={true}` (default) |
| Non-streaming | `await runtime.chat(body)` | `streaming={false}` |

---

## CORS

If your frontend and backend are on different domains:

<Tabs items={['Next.js', 'Hono', 'Express']}>
  <Tab value="Next.js">
    ```ts title="app/api/chat/route.ts"
    const corsHeaders = {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type',
    };

    export async function OPTIONS() {
      return new Response(null, { headers: corsHeaders });
    }

    export async function POST(req: Request) {
      const body = await req.json();
      const response = runtime.stream(body).toResponse();

      Object.entries(corsHeaders).forEach(([key, value]) => {
        response.headers.set(key, value);
      });

      return response;
    }
    ```
  </Tab>
  <Tab value="Hono">
    ```ts
    import { cors } from 'hono/cors';

    const app = createHonoApp(runtime);
    app.use('*', cors());
    ```
  </Tab>
  <Tab value="Express">
    ```ts
    import cors from 'cors';

    app.use(cors());
    ```
  </Tab>
</Tabs>

---

## Next Steps

- [Server Setup](/docs/server) — Full runtime configuration and response methods
- [Tools](/docs/tools) — Add function calling to your Copilot
- [Providers](/docs/providers) — Configure different LLM providers
