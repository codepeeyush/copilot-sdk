---
title: Google (Gemini)
description: Use Gemini 1.5 Pro, Gemini Flash models
---

import { Callout } from 'fumadocs-ui/components/callout';

<ProviderHeader
  provider="google"
  name="Google Generative AI"
  description="Gemini 1.5 Pro, Gemini Flash"
/>

Google's Gemini models. Excellent for multimodal tasks with massive context windows.

---

## Setup

### 1. Get API Key

Get your API key from [Google AI Studio](https://makersuite.google.com/app/apikey)

### 2. Add Environment Variable

```bash
# .env.local
GOOGLE_API_KEY=...
```

### 3. Configure Provider

```tsx
<CopilotProvider
  runtimeUrl="/api/chat"
  llm={{
    provider: 'google',
    model: 'gemini-1.5-pro',
  }}
>
  <CopilotChat />
</CopilotProvider>
```

---

## Available Models

| Model | Context | Best For |
|-------|---------|----------|
| `gemini-1.5-pro` | 1M+ | Complex tasks, huge context |
| `gemini-1.5-flash` | 1M+ | Fast, efficient |
| `gemini-1.0-pro` | 32K | Previous generation |

<Callout type="info">
**Recommended:** Use `gemini-1.5-flash` for speed or `gemini-1.5-pro` for complex tasks.
</Callout>

---

## Configuration Options

```tsx
llm={{
  provider: 'google',
  model: 'gemini-1.5-pro',
  temperature: 0.7,        // 0-1
  maxTokens: 8192,         // Max response length
  topP: 0.95,              // Nucleus sampling
  topK: 40,                // Top-k sampling
}}
```

---

## Massive Context Window

Gemini supports 1M+ token context:

```tsx
// Process entire codebases or books
<CopilotProvider
  systemPrompt={`Here is the entire codebase:

  ${entireCodebase}  // Can be 500K+ tokens!

  Help users understand and modify this code.`}
>
```

---

## Multimodal (Images, Video, Audio)

Gemini excels at multimodal understanding:

```tsx
const { sendMessage } = useCopilot();

// Image analysis
sendMessage("What's in this image?", [
  { type: 'image', data: imageBase64, mimeType: 'image/png' }
]);

// Video analysis (if supported)
sendMessage("Summarize this video", [
  { type: 'video', data: videoBase64, mimeType: 'video/mp4' }
]);
```

---

## Tool Calling

```tsx
useToolWithSchema({
  name: 'search_knowledge',
  description: 'Search internal knowledge base',
  schema: z.object({
    query: z.string(),
    filters: z.object({
      category: z.string().optional(),
      dateRange: z.string().optional(),
    }).optional(),
  }),
  handler: async ({ query, filters }) => {
    const results = await searchKnowledge(query, filters);
    return { success: true, data: results };
  },
});
```

---

## Pricing

| Model | Input | Output |
|-------|-------|--------|
| gemini-1.5-pro | $1.25/1M tokens | $5/1M tokens |
| gemini-1.5-flash | $0.075/1M tokens | $0.30/1M tokens |

*Very competitive pricing. Check [Google AI pricing](https://ai.google.dev/pricing) for current rates.*

---

## Next Steps

- [Groq](/docs/providers/groq) - Ultra-fast inference
- [Features](/docs/features) - Explore SDK features
