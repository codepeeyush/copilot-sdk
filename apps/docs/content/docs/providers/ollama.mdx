---
title: Ollama
description: Run models locally on your machine
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';

<ProviderHeader
  provider="ollama"
  name="Ollama"
  description="Run models locally - free & private"
/>

Run open-source LLMs locally. **Free, private, no API keys needed.**

<Callout type="info">
Ollama is perfect for development, testing, or applications requiring data privacy.
</Callout>

---

## Setup

### 1. Install Ollama

<Tabs items={['macOS', 'Linux', 'Windows']}>
  <Tab value="macOS">
    ```bash
    brew install ollama
    ```
    Or download from [ollama.ai](https://ollama.ai)
  </Tab>
  <Tab value="Linux">
    ```bash
    curl -fsSL https://ollama.ai/install.sh | sh
    ```
  </Tab>
  <Tab value="Windows">
    Download from [ollama.ai](https://ollama.ai)
  </Tab>
</Tabs>

### 2. Pull a Model

```bash
# Llama 3.1 (recommended)
ollama pull llama3.1

# Or smaller models
ollama pull llama3.1:8b
ollama pull mistral
ollama pull codellama
```

### 3. Start Ollama Server

```bash
ollama serve
# Server runs on http://localhost:11434
```

### 4. Configure Provider

```tsx
<CopilotProvider
  runtimeUrl="/api/chat"
  llm={{
    provider: 'ollama',
    model: 'llama3.1',
    baseUrl: 'http://localhost:11434',
  }}
>
  <CopilotChat />
</CopilotProvider>
```

---

## Available Models

| Model | Size | RAM Needed | Best For |
|-------|------|------------|----------|
| `llama3.1` | 8B | 8GB | General use |
| `llama3.1:70b` | 70B | 48GB | High quality |
| `mistral` | 7B | 8GB | Fast |
| `codellama` | 7B | 8GB | Code |
| `llava` | 7B | 8GB | Vision |

<Callout type="warning">
Larger models need more RAM. Start with 8B models if you have 16GB RAM.
</Callout>

---

## Configuration Options

```tsx
llm={{
  provider: 'ollama',
  model: 'llama3.1',
  baseUrl: 'http://localhost:11434',
  temperature: 0.7,
  maxTokens: 4096,
  // Ollama-specific options
  numCtx: 4096,           // Context window
  numPredict: 128,        // Max tokens to predict
}}
```

---

## Use Cases

### Local Development

No API costs during development:

```tsx
// Use Ollama locally, switch to OpenAI in production
const provider = process.env.NODE_ENV === 'development'
  ? { provider: 'ollama', model: 'llama3.1', baseUrl: 'http://localhost:11434' }
  : { provider: 'openai', model: 'gpt-4o' };
```

### Data Privacy

All data stays on your machine:

```tsx
// Sensitive data never leaves your network
<CopilotProvider
  llm={{ provider: 'ollama', model: 'llama3.1' }}
  systemPrompt="Process this confidential data..."
>
```

### Offline Usage

Works without internet:

```tsx
// Perfect for air-gapped environments
<CopilotProvider
  llm={{
    provider: 'ollama',
    model: 'llama3.1',
    baseUrl: 'http://internal-server:11434',
  }}
>
```

---

## Tool Calling

Llama 3.1 supports function calling:

```tsx
useToolWithSchema({
  name: 'local_search',
  description: 'Search local files',
  schema: z.object({
    query: z.string(),
    path: z.string().optional(),
  }),
  handler: async ({ query, path }) => {
    // Search runs locally too
    const results = await searchLocalFiles(query, path);
    return { success: true, data: results };
  },
});
```

---

## Performance Tips

1. **Use GPU acceleration** - Ollama auto-detects NVIDIA/Apple Silicon
2. **Quantized models** - Use `:q4` variants for faster inference
3. **Adjust context** - Lower `numCtx` for faster responses

```bash
# Run with less memory
ollama run llama3.1:8b-q4_0
```

---

## Runtime Configuration

```ts
// app/api/chat/route.ts
const runtime = createRuntime({
  providers: {
    ollama: {
      baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
    },
  },
});
```

---

## Next Steps

- [Custom Provider](/docs/providers/custom-provider) - Build your own
- [Architecture](/docs/developers/architecture) - How it works
