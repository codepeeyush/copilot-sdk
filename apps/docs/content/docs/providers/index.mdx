---
title: Providers Overview
description: Connect to any LLM provider
---

import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';

<div className="not-prose mb-8 rounded-xl overflow-hidden border bg-gradient-to-br from-cyan-500/5 via-blue-500/10 to-indigo-500/5">
  <div className="aspect-[21/9] flex items-center justify-center">
    <div className="text-center">
      <div className="text-6xl mb-4">ðŸ”Œ</div>
      <p className="text-muted-foreground">Provider logos grid placeholder</p>
    </div>
  </div>
</div>

# Providers

YourGPT Copilot SDK supports multiple LLM providers out of the box. **Switch providers without changing your frontend code.**

<Callout type="info">
All providers use the same API. Change one line in your config to switch from OpenAI to Anthropic.
</Callout>

---

## Supported Providers

<Cards>
  <Card title="OpenAI" href="/docs/providers/openai" icon="ðŸŸ¢">
    GPT-4o, GPT-4, GPT-3.5 Turbo
  </Card>
  <Card title="Anthropic" href="/docs/providers/anthropic" icon="ðŸŸ ">
    Claude 3.5 Sonnet, Claude 3 Opus
  </Card>
  <Card title="Google" href="/docs/providers/google" icon="ðŸ”µ">
    Gemini 1.5 Pro, Gemini Flash
  </Card>
  <Card title="Groq" href="/docs/providers/groq" icon="âš¡">
    Llama 3.1, Mixtral (ultra-fast)
  </Card>
  <Card title="Mistral" href="/docs/providers/mistral" icon="ðŸŸ£">
    Mistral Large, Medium, Small
  </Card>
  <Card title="Azure OpenAI" href="/docs/providers/azure" icon="â˜ï¸">
    Enterprise OpenAI deployment
  </Card>
  <Card title="Ollama" href="/docs/providers/ollama" icon="ðŸ¦™">
    Run models locally
  </Card>
  <Card title="Custom Provider" href="/docs/providers/custom-provider" icon="âš™ï¸">
    Build your own adapter
  </Card>
</Cards>

---

## Quick Start

### 1. Choose Your Provider

```tsx
<YourGPTProvider
  runtimeUrl="/api/chat"
  llm={{
    provider: 'openai',    // or 'anthropic', 'google', 'groq', etc.
    model: 'gpt-4o',
  }}
>
  <CopilotChat />
</YourGPTProvider>
```

### 2. Add API Key

```bash
# .env.local
OPENAI_API_KEY=sk-...
```

### 3. That's It!

The runtime automatically detects and uses the correct provider.

---

## Switching Providers

Change one line - everything else stays the same:

```tsx
// OpenAI
llm={{ provider: 'openai', model: 'gpt-4o' }}

// Anthropic
llm={{ provider: 'anthropic', model: 'claude-3-5-sonnet-20241022' }}

// Google
llm={{ provider: 'google', model: 'gemini-1.5-pro' }}

// Groq (fastest)
llm={{ provider: 'groq', model: 'llama-3.1-70b-versatile' }}
```

<Callout type="info">
Your tools, UI, and all frontend code remain unchanged. The SDK normalizes responses across providers.
</Callout>

---

## Provider Comparison

| Provider | Speed | Quality | Cost | Best For |
|----------|-------|---------|------|----------|
| OpenAI | Fast | Excellent | $$ | General use |
| Anthropic | Medium | Excellent | $$ | Long context, safety |
| Google | Fast | Very Good | $ | Multimodal |
| Groq | Ultra Fast | Good | $ | Speed-critical apps |
| Mistral | Fast | Very Good | $ | European compliance |
| Ollama | Varies | Varies | Free | Local/private |

---

## Need a Custom Provider?

<Cards>
  <Card title="Custom Provider Guide" href="/docs/providers/custom-provider" icon="ðŸ”§">
    Build an adapter for any LLM API
  </Card>
</Cards>
