---
title: Providers Overview
description: Connect to any LLM provider
---

import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';

Copilot SDK supports multiple LLM providers out of the box. **Switch providers without changing your frontend code.**

<Callout type="info">
All providers use the same API. Change one line in your config to switch from OpenAI to Anthropic.
</Callout>

<ProviderCards />

---

## Quick Start

### 1. Choose Your Provider

```tsx
<CopilotProvider
  runtimeUrl="/api/chat"
  llm={{
    provider: 'openai',    // or 'anthropic', 'google', 'groq', etc.
    model: 'gpt-4o',
  }}
>
  <CopilotChat />
</CopilotProvider>
```

### 2. Add API Key

```bash
# .env.local
OPENAI_API_KEY=sk-...
```

### 3. That's It!

The runtime automatically detects and uses the correct provider.

---

## Switching Providers

Change one line - everything else stays the same:

```tsx
// OpenAI
llm={{ provider: 'openai', model: 'gpt-4o' }}

// Anthropic
llm={{ provider: 'anthropic', model: 'claude-3-5-sonnet-20241022' }}

// Google
llm={{ provider: 'google', model: 'gemini-1.5-pro' }}

// Groq (fastest)
llm={{ provider: 'groq', model: 'llama-3.1-70b-versatile' }}
```

<Callout type="info">
Your tools, UI, and all frontend code remain unchanged. The SDK normalizes responses across providers.
</Callout>

---

## Provider Comparison

| Provider | Speed | Quality | Cost | Best For |
|----------|-------|---------|------|----------|
| OpenAI | Fast | Excellent | $$ | General use |
| Anthropic | Medium | Excellent | $$ | Long context, safety |
| Google | Fast | Very Good | $ | Multimodal |
| Groq | Ultra Fast | Good | $ | Speed-critical apps |
| Mistral | Fast | Very Good | $ | European compliance |
| Ollama | Varies | Varies | Free | Local/private |

---

## Need a Custom Provider?

<Cards>
  <Card title="Custom Provider Guide" href="/docs/providers/custom-provider" icon="ðŸ”§">
    Build an adapter for any LLM API
  </Card>
</Cards>
