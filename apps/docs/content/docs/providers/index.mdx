---
title: Providers Overview
description: Connect to any LLM provider
---

import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';

Copilot SDK supports multiple LLM providers through `@yourgpt/llm-sdk`. **Switch providers without changing your frontend code.**

<Callout type="info">
All providers use the same API. Change one line in your backend to switch from OpenAI to Anthropic.
</Callout>

<ProviderCards />

---

## How It Works

Install the SDK and the provider's official SDK. Each provider returns a model instance that works with `generateText()` and `streamText()`.

### Installation

<Tabs items={['OpenAI / Google / xAI', 'Anthropic']}>
  <Tab value="OpenAI / Google / xAI">
    ```bash
    npm install @yourgpt/copilot-sdk @yourgpt/llm-sdk openai
    ```
    <Callout type="info">
    OpenAI, Google Gemini, and xAI all use the `openai` SDK (OpenAI-compatible APIs).
    </Callout>
  </Tab>
  <Tab value="Anthropic">
    ```bash
    npm install @yourgpt/copilot-sdk @yourgpt/llm-sdk @anthropic-ai/sdk
    ```
  </Tab>
</Tabs>

### Backend Setup

```ts title="app/api/chat/route.ts"
import { streamText } from '@yourgpt/llm-sdk';
import { openai } from '@yourgpt/llm-sdk/openai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: openai('gpt-5.2'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toTextStreamResponse();
}
```

### Frontend Setup

```tsx title="app/providers.tsx"
'use client';

import { CopilotProvider } from '@yourgpt/copilot-sdk/react';

export function Providers({ children }: { children: React.ReactNode }) {
  return (
    <CopilotProvider runtimeUrl="/api/chat">
      {children}
    </CopilotProvider>
  );
}
```

---

## Switching Providers

Change the import and model - frontend stays the same:

```ts title="app/api/chat/route.ts"
import { streamText } from '@yourgpt/llm-sdk';

// OpenAI
import { openai } from '@yourgpt/llm-sdk/openai';
const model = openai('gpt-4o');

// Anthropic
import { anthropic } from '@yourgpt/llm-sdk/anthropic';
const model = anthropic('claude-3-5-sonnet-20241022');

// Google
import { google } from '@yourgpt/llm-sdk/google';
const model = google('gemini-2.0-flash');

// xAI (Grok)
import { xai } from '@yourgpt/llm-sdk/xai';
const model = xai('grok-3-fast');

// Use any model
const result = await streamText({
  model,
  messages,
});
```

<Callout type="info">
Your tools, UI, and all frontend code remain unchanged. The SDK normalizes responses across providers.
</Callout>

---

## Available Providers

| Provider | Import | SDK Required | Example |
|----------|--------|--------------|---------|
| OpenAI | `@yourgpt/llm-sdk/openai` | `openai` | `openai('gpt-4o')` |
| Anthropic | `@yourgpt/llm-sdk/anthropic` | `@anthropic-ai/sdk` | `anthropic('claude-3-5-sonnet-20241022')` |
| Google | `@yourgpt/llm-sdk/google` | `openai` | `google('gemini-2.0-flash')` |
| xAI | `@yourgpt/llm-sdk/xai` | `openai` | `xai('grok-3-fast')` |

---

## Why Only 2 SDKs?

Most LLM providers now offer **OpenAI-compatible APIs**. This means you only need:

| SDK | Providers |
|-----|-----------|
| `openai` | OpenAI, Google Gemini, xAI Grok, Azure OpenAI, Groq, Together AI, Ollama |
| `@anthropic-ai/sdk` | Anthropic Claude (native SDK for full features) |

<Callout type="info">
Google and xAI use OpenAI-compatible endpoints. We automatically configure the correct `baseURL` for each provider.
</Callout>

---

## Provider Comparison

| Provider | Speed | Quality | Cost | Best For |
|----------|-------|---------|------|----------|
| OpenAI | Fast | Excellent | $$ | General use |
| Anthropic | Medium | Excellent | $$ | Long context, safety |
| Google | Fast | Very Good | $ | Multimodal |
| xAI | Ultra Fast | Excellent | $ | Speed-critical apps |
