---
title: Runtime (Server)
description: Server-side API handler for chat requests
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';

# Runtime

The server-side handler. Talks to LLMs, handles the agentic loop, streams responses.

---

## Quick Setup

```ts title="app/api/chat/route.ts"
import { createRuntime, AnthropicAdapter } from '@yourgpt/copilot-sdk-runtime';

const runtime = createRuntime({
  adapter: new AnthropicAdapter({
    apiKey: process.env.ANTHROPIC_API_KEY!,
    model: 'claude-sonnet-4-20250514',
  }),
  systemPrompt: 'You are a helpful assistant.',
});

export async function POST(request: Request) {
  return runtime.handleRequest(request);
}
```

That's it. The runtime handles:
- Parsing incoming messages
- Calling the LLM
- Processing tool calls
- Streaming responses via SSE
- Running the agentic loop

---

## Adapters

<Tabs items={['Anthropic', 'OpenAI', 'Groq', 'Ollama']}>
  <Tab value="Anthropic">
    ```ts
    import { AnthropicAdapter } from '@yourgpt/copilot-sdk-runtime';

    new AnthropicAdapter({
      apiKey: process.env.ANTHROPIC_API_KEY!,
      model: 'claude-sonnet-4-20250514',
      // Optional
      maxTokens: 4096,
      temperature: 0.7,
    })
    ```

    **Models:** `claude-sonnet-4-20250514`, `claude-opus-4-20250514`, `claude-3-5-haiku-20241022`
  </Tab>
  <Tab value="OpenAI">
    ```ts
    import { OpenAIAdapter } from '@yourgpt/copilot-sdk-runtime';

    new OpenAIAdapter({
      apiKey: process.env.OPENAI_API_KEY!,
      model: 'gpt-4o-mini',
      // Optional
      maxTokens: 4096,
      temperature: 0.7,
    })
    ```

    **Models:** `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `o1-mini`
  </Tab>
  <Tab value="Groq">
    ```ts
    import { GroqAdapter } from '@yourgpt/copilot-sdk-runtime';

    new GroqAdapter({
      apiKey: process.env.GROQ_API_KEY!,
      model: 'llama-3.3-70b-versatile',
      // Optional
      maxTokens: 4096,
      temperature: 0.7,
    })
    ```

    **Models:** `llama-3.3-70b-versatile`, `mixtral-8x7b-32768`, `gemma2-9b-it`
  </Tab>
  <Tab value="Ollama">
    ```ts
    import { OllamaAdapter } from '@yourgpt/copilot-sdk-runtime';

    new OllamaAdapter({
      baseUrl: 'http://localhost:11434',
      model: 'llama3.2',
      // Optional
      maxTokens: 4096,
      temperature: 0.7,
    })
    ```

    **Models:** Any model available in your Ollama instance
  </Tab>
</Tabs>

### Factory Functions

You can also use factory functions:

```ts
import {
  createOpenAIAdapter,
  createAnthropicAdapter,
  createGroqAdapter,
  createOllamaAdapter,
} from '@yourgpt/copilot-sdk-runtime';

const adapter = createAnthropicAdapter({
  apiKey: process.env.ANTHROPIC_API_KEY!,
  model: 'claude-sonnet-4-20250514',
});
```

---

## System Prompt

Tell the AI who it is and what it can do:

```ts
const runtime = createRuntime({
  adapter: new AnthropicAdapter({ ... }),
  systemPrompt: `You are a customer support agent for Acme Corp.

You can help users with:
- Account questions
- Billing issues
- Product support

Be friendly but professional. If you don't know something, say so.

Available tools:
- search_help: Find answers in our knowledge base
- create_ticket: Create a support ticket
- get_order: Look up order details`,
});
```

<Callout type="info">
The system prompt is where you define the AI's personality and capabilities. Make it specific!
</Callout>

---

## Configuration Options

```ts
const runtime = createRuntime({
  adapter: new AnthropicAdapter({ ... }),

  systemPrompt: 'You are helpful.',

  // Max iterations in the agentic loop (default: 10)
  maxIterations: 10,

  // Custom headers for requests
  headers: {
    'X-Custom-Header': 'value',
  },
});
```

---

## How the Agentic Loop Works

```
Client sends: { messages, tools }
         ↓
Runtime calls LLM with system prompt + messages + tools
         ↓
LLM responds with text OR tool_calls
         ↓
If tool_calls:
  → Runtime sends tool_calls event to client
  → Client executes tools, sends results back
  → Runtime continues with results
  → Loop until LLM responds with text (or max iterations)
         ↓
Stream final response to client
```

The runtime handles all of this. You don't need to manage the loop yourself.

### runAgentLoop

For advanced use cases, you can run the agent loop directly:

```ts
import { runAgentLoop, DEFAULT_MAX_ITERATIONS } from '@yourgpt/copilot-sdk-runtime';

const result = await runAgentLoop({
  adapter,
  messages,
  tools,
  systemPrompt,
  maxIterations: DEFAULT_MAX_ITERATIONS, // 10
  onToolCall: async (toolCall) => {
    // Handle tool execution
    return { success: true, data: {} };
  },
  onStream: (event) => {
    // Handle streaming events
  },
});
```

---

## SSE Events

The runtime streams these events to the client:

| Event | Description |
|-------|-------------|
| `message_start` | Stream started |
| `message_delta` | Text chunks as they're generated |
| `message_end` | Message complete |
| `tool_calls` | Tools the AI wants to execute |
| `tool_status` | Tool execution status update |
| `tool_result` | Results of tool execution |
| `loop_iteration` | Agent loop iteration info |
| `loop_complete` | Agent loop finished |
| `sources` | Knowledge base citations |
| `done` | Stream complete |
| `error` | Something went wrong |

---

## Framework Integrations

### Next.js (App Router)

```ts title="app/api/chat/route.ts"
import { createRuntime, AnthropicAdapter } from '@yourgpt/copilot-sdk-runtime';

const runtime = createRuntime({
  adapter: new AnthropicAdapter({ ... }),
  systemPrompt: '...',
});

export async function POST(request: Request) {
  return runtime.handleRequest(request);
}
```

### Next.js (Pages Router)

```ts title="pages/api/chat.ts"
import { createNextHandler, createRuntime } from '@yourgpt/copilot-sdk-runtime';

const runtime = createRuntime({ ... });

export default createNextHandler(runtime);
```

### Express

```ts
import express from 'express';
import { createRuntime, createExpressMiddleware } from '@yourgpt/copilot-sdk-runtime';

const app = express();
const runtime = createRuntime({ ... });

app.use('/api/chat', createExpressMiddleware(runtime));
```

### Hono

```ts
import { createHonoApp, createRuntime } from '@yourgpt/copilot-sdk-runtime';

const runtime = createRuntime({ ... });
const app = createHonoApp(runtime);

export default app;
```

### Node.js HTTP

```ts
import http from 'http';
import { createNodeHandler, createRuntime } from '@yourgpt/copilot-sdk-runtime';

const runtime = createRuntime({ ... });
const handler = createNodeHandler(runtime);

const server = http.createServer(handler);
server.listen(3000);
```

---

## Streaming Utilities

For custom streaming implementations:

```ts
import {
  createSSEHeaders,
  formatSSEData,
  createEventStream,
  createSSEResponse,
} from '@yourgpt/copilot-sdk-runtime';

// Get standard SSE headers
const headers = createSSEHeaders();

// Format data as SSE
const sseData = formatSSEData({ type: 'message_delta', content: 'Hello' });

// Create event stream
const stream = createEventStream(async function* () {
  yield { type: 'message_start' };
  yield { type: 'message_delta', content: 'Hello' };
  yield { type: 'done' };
});

// Create full SSE response
const response = createSSEResponse(stream);
```

---

## Provider Formatters

For working with different LLM providers' tool formats:

```ts
import {
  getFormatter,
  isProviderSupported,
  getSupportedProviders,
  anthropicFormatter,
  openaiFormatter,
  geminiFormatter,
} from '@yourgpt/copilot-sdk-runtime';

// Get formatter for a provider
const formatter = getFormatter('anthropic');

// Format tool for provider
const anthropicTool = formatter.formatTool(toolDefinition);

// Parse tool call from provider response
const toolCall = formatter.parseToolCall(providerResponse);

// Format tool result for provider
const toolResult = formatter.formatToolResult(result);

// Check provider support
if (isProviderSupported('anthropic')) {
  // ...
}

// Get all supported providers
const providers = getSupportedProviders(); // ['anthropic', 'openai', 'gemini']
```

---

## Knowledge Base (Server-side)

Integrate with YourGPT Knowledge Base:

```ts
import {
  searchKnowledgeBase,
  formatKnowledgeResultsForAI,
  KNOWLEDGE_BASE_SYSTEM_INSTRUCTION,
} from '@yourgpt/copilot-sdk-runtime';

// Search knowledge base
const results = await searchKnowledgeBase({
  projectUid: 'your-project-id',
  query: 'how to reset password',
  token: 'your-api-token',
  limit: 5,
});

// Format results for LLM context
const context = formatKnowledgeResultsForAI(results);

// Add to system prompt
const systemPrompt = `${KNOWLEDGE_BASE_SYSTEM_INSTRUCTION}

${context}

You are a helpful assistant...`;
```

---

## Custom Request Handling

Need more control? Access the raw request:

```ts
export async function POST(request: Request) {
  const body = await request.json();

  // Add custom context
  const customContext = await getCustomContext(body.userId);

  // Modify system prompt based on user
  const runtime = createRuntime({
    adapter: new AnthropicAdapter({ ... }),
    systemPrompt: `You are helping ${customContext.userName}...`,
  });

  return runtime.handleRequest(request, {
    additionalContext: customContext,
  });
}
```

---

## Error Handling

```ts
export async function POST(request: Request) {
  try {
    return await runtime.handleRequest(request);
  } catch (error) {
    console.error('Chat error:', error);
    return new Response(
      JSON.stringify({ error: 'Something went wrong' }),
      { status: 500 }
    );
  }
}
```

---

## Multi-Provider Setup

Swap providers based on conditions:

```ts
import {
  OpenAIAdapter,
  AnthropicAdapter,
  GroqAdapter,
  createRuntime,
} from '@yourgpt/copilot-sdk-runtime';

function getAdapter(model: string) {
  if (model.startsWith('claude')) {
    return new AnthropicAdapter({
      apiKey: process.env.ANTHROPIC_API_KEY!,
      model,
    });
  }
  if (model.startsWith('gpt')) {
    return new OpenAIAdapter({
      apiKey: process.env.OPENAI_API_KEY!,
      model,
    });
  }
  if (model.startsWith('llama') || model.startsWith('mixtral')) {
    return new GroqAdapter({
      apiKey: process.env.GROQ_API_KEY!,
      model,
    });
  }
  throw new Error(`Unknown model: ${model}`);
}

export async function POST(request: Request) {
  const { model, ...rest } = await request.json();

  const runtime = createRuntime({
    adapter: getAdapter(model),
    systemPrompt: '...',
  });

  const newRequest = new Request(request.url, {
    method: 'POST',
    body: JSON.stringify(rest),
  });

  return runtime.handleRequest(newRequest);
}
```

---

## All Exports

```ts
// Server
export { Runtime, createRuntime } from '@yourgpt/copilot-sdk-runtime';

// Framework integrations
export {
  createHonoApp,
  createNextHandler,
  createExpressMiddleware,
  createNodeHandler,
} from '@yourgpt/copilot-sdk-runtime';

// Streaming utilities
export {
  createSSEHeaders,
  formatSSEData,
  createEventStream,
  createSSEResponse,
} from '@yourgpt/copilot-sdk-runtime';

// Agent loop
export { runAgentLoop, DEFAULT_MAX_ITERATIONS } from '@yourgpt/copilot-sdk-runtime';

// Knowledge Base
export {
  searchKnowledgeBase,
  formatKnowledgeResultsForAI,
  KNOWLEDGE_BASE_SYSTEM_INSTRUCTION,
} from '@yourgpt/copilot-sdk-runtime';

// Adapters
export {
  OpenAIAdapter,
  createOpenAIAdapter,
  AnthropicAdapter,
  createAnthropicAdapter,
  GroqAdapter,
  createGroqAdapter,
  OllamaAdapter,
  createOllamaAdapter,
} from '@yourgpt/copilot-sdk-runtime';

// Provider formatters
export {
  getFormatter,
  isProviderSupported,
  getSupportedProviders,
  anthropicFormatter,
  openaiFormatter,
  geminiFormatter,
} from '@yourgpt/copilot-sdk-runtime';
```
