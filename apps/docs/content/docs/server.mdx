---
title: Server Setup
description: Configure your backend API for the Copilot SDK
icon: Server
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';

Set up your backend API to handle chat requests from the Copilot SDK.

---

## Overview

The Copilot SDK frontend connects to your backend API endpoint. Your server:

1. Receives chat messages from the frontend
2. Calls the LLM with your configuration
3. Streams the response back to the client

<ServerFlowDiagram />

---

## REST API Contract

### Request

**Endpoint:** `POST /api/chat`

```json
{
  "messages": [
    { "role": "user", "content": "Hello!" }
  ]
}
```

### Response

The SDK supports two response formats:

<Tabs items={['Text Stream', 'Data Stream']}>
  <Tab value="Text Stream">
    Simple text streaming for basic chat (no tools).

    **Content-Type:** `text/plain; charset=utf-8`

    ```
    Hello! How can I help you today?
    ```

    Use `result.toTextStreamResponse()` to return this format.
  </Tab>
  <Tab value="Data Stream">
    SSE format with structured events. Use when you need tools, usage info, or step-by-step data.

    **Content-Type:** `text/event-stream`

    ```
    data: {"type":"text-delta","text":"Hello"}
    data: {"type":"text-delta","text":"!"}
    data: {"type":"finish","finishReason":"stop","usage":{"promptTokens":10,"completionTokens":5}}
    data: [DONE]
    ```

    Use `result.toDataStreamResponse()` to return this format.
  </Tab>
</Tabs>

---

## Framework Examples

<Tabs items={['Next.js', 'Express', 'Node.js']}>
  <Tab value="Next.js">
    ```ts title="app/api/chat/route.ts"
    import { streamText } from '@yourgpt/llm-sdk';
    import { openai } from '@yourgpt/llm-sdk/openai';

    export async function POST(req: Request) {
      const { messages } = await req.json();

      const result = await streamText({
        model: openai('gpt-4o'),
        system: 'You are a helpful assistant.',
        messages,
      });

      return result.toTextStreamResponse();
    }
    ```
  </Tab>
  <Tab value="Express">
    ```ts title="server.ts"
    import express from 'express';
    import cors from 'cors';
    import { createRuntime } from '@yourgpt/llm-sdk';
import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const app = express();
    app.use(cors());
    app.use(express.json());

    // Create runtime once at startup
    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });

    // Chat endpoint
    app.post('/api/chat', async (req, res) => {
      // Convert Express request to Web Request
      const webRequest = new Request('http://localhost/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(req.body),
      });

      // Handle with runtime
      const response = await runtime.handleRequest(webRequest);

      // Stream response back to client
      response.headers.forEach((value, key) => res.setHeader(key, value));
      res.status(response.status);

      if (response.body) {
        const reader = response.body.getReader();
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          res.write(new TextDecoder().decode(value));
        }
      }
      res.end();
    });

    app.listen(3001, () => console.log('Server on http://localhost:3001'));
    ```
  </Tab>
  <Tab value="Node.js">
    ```ts title="server.ts"
    import { createServer } from 'http';
    import { streamText } from '@yourgpt/llm-sdk';
    import { openai } from '@yourgpt/llm-sdk/openai';

    createServer(async (req, res) => {
      if (req.method === 'POST' && req.url === '/api/chat') {
        const body = await getBody(req);
        const { messages } = JSON.parse(body);

        const result = await streamText({
          model: openai('gpt-4o'),
          system: 'You are a helpful assistant.',
          messages,
        });

        const response = result.toTextStreamResponse();
        res.writeHead(200, Object.fromEntries(response.headers));

        const reader = response.body!.getReader();
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          res.write(value);
        }
        res.end();
      }
    }).listen(3001);

    function getBody(req: any): Promise<string> {
      return new Promise((resolve) => {
        let data = '';
        req.on('data', (chunk: any) => data += chunk);
        req.on('end', () => resolve(data));
      });
    }
    ```
  </Tab>
</Tabs>

---

## With Tools

Add tools to let the AI call functions on your server:

```ts title="app/api/chat/route.ts"
import { streamText, tool } from '@yourgpt/llm-sdk';
import { openai } from '@yourgpt/llm-sdk/openai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
    tools: {
      getWeather: tool({
        description: 'Get current weather for a city',
        parameters: z.object({
          city: z.string().describe('City name'),
        }),
        execute: async ({ city }) => {
          const data = await fetchWeatherAPI(city);
          return { temperature: data.temp, condition: data.condition };
        },
      }),
      searchProducts: tool({
        description: 'Search the product database',
        parameters: z.object({
          query: z.string(),
          limit: z.number().optional().default(10),
        }),
        execute: async ({ query, limit }) => {
          return await db.products.search(query, limit);
        },
      }),
    },
    maxSteps: 5,
  });

  return result.toDataStreamResponse();
}
```

<Callout type="info">
Use `toDataStreamResponse()` when using tools to stream structured events including tool calls and results.
</Callout>

---

## Runtime API (Advanced)

For more control over the server, use `createRuntime()` instead of `streamText()`:

```ts title="app/api/chat/route.ts"
import { createRuntime } from '@yourgpt/llm-sdk';
import { createOpenAI } from '@yourgpt/llm-sdk/openai';

const runtime = createRuntime({
  provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
  model: 'gpt-4o',
  systemPrompt: 'You are a helpful assistant.',
  agentLoop: {
    maxIterations: 20,  // Max tool call cycles
    debug: true,        // Enable debug logging
  },
  tools: [/* server-side tools */],
});

export async function POST(request: Request) {
  return runtime.handleRequest(request);
}
```

### Runtime Configuration

| Option | Type | Description |
|--------|------|-------------|
| `provider` | `AIProvider` | Provider instance from `createOpenAI()`, `createAnthropic()`, etc. |
| `model` | `string` | Model ID (e.g., `'gpt-4o'`, `'claude-sonnet-4-20250514'`) |
| `systemPrompt` | `string` | Default system prompt |
| `agentLoop.maxIterations` | `number` | Max tool execution cycles (default: 20) |
| `agentLoop.debug` | `boolean` | Enable debug logging |
| `tools` | `ToolDefinition[]` | Server-side tools |
| `toolContext` | `Record<string, unknown>` | Context data passed to all tool handlers |
| `debug` | `boolean` | Enable request/response logging |

### Server-Side Persistence

Use the `onFinish` callback to persist messages after each request:

```ts title="app/api/chat/route.ts"
export async function POST(request: Request) {
  return runtime.handleRequest(request, {
    onFinish: async ({ messages, threadId }) => {
      // Save to your database
      await db.thread.upsert({
        where: { id: threadId },
        update: { messages, updatedAt: new Date() },
        create: { id: threadId, messages },
      });
    },
  });
}
```

### Tool Context

Pass authentication or context data to all tool handlers:

```ts
const runtime = createRuntime({
  provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
  model: 'gpt-4o',
  toolContext: {
    userId: 'user_123',
    tenantId: 'tenant_456',
  },
  tools: [
    {
      name: 'get_user_data',
      description: 'Get data for the current user',
      location: 'server',
      inputSchema: { type: 'object', properties: {}, required: [] },
      handler: async (args, context) => {
        // Access context.data.userId, context.data.tenantId
        // Also available: context.headers, context.request, context.threadId
        const user = await db.user.findById(context.data.userId);
        return { success: true, data: user };
      },
    },
  ],
});
```

### Provider Examples

<Tabs items={['OpenAI', 'Anthropic', 'Google']}>
  <Tab value="OpenAI">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
    });
    ```
  </Tab>
  <Tab value="Anthropic">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createAnthropic } from '@yourgpt/llm-sdk/anthropic';

    const runtime = createRuntime({
      provider: createAnthropic({ apiKey: process.env.ANTHROPIC_API_KEY }),
      model: 'claude-sonnet-4-20250514',
    });
    ```

    <Callout type="info">
    Requires `@anthropic-ai/sdk` package: `npm install @anthropic-ai/sdk`
    </Callout>
  </Tab>
  <Tab value="Google">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createGoogle } from '@yourgpt/llm-sdk/google';

    const runtime = createRuntime({
      provider: createGoogle({ apiKey: process.env.GOOGLE_API_KEY }),
      model: 'gemini-2.0-flash',
    });
    ```

    <Callout type="info">
    Google uses OpenAI-compatible API. Requires `openai` package.
    </Callout>
  </Tab>
</Tabs>

---

## Environment Variables

Store your API keys in environment variables:

```bash title=".env.local"
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_AI_API_KEY=...
```

Access them in your API route:

```ts
import { openai } from '@yourgpt/llm-sdk/openai';

// API key is read from OPENAI_API_KEY automatically
const model = openai('gpt-4o');

// Or pass explicitly
const model = openai('gpt-4o', {
  apiKey: process.env.OPENAI_API_KEY,
});
```

---

## CORS Configuration

For cross-origin requests (e.g., frontend on different port):

<Tabs items={['Next.js', 'Node.js']}>
  <Tab value="Next.js">
    ```ts title="app/api/chat/route.ts"
    export async function OPTIONS() {
      return new Response(null, {
        headers: {
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type',
        },
      });
    }

    export async function POST(req: Request) {
      // ... your handler

      const response = result.toTextStreamResponse();

      // Add CORS headers
      response.headers.set('Access-Control-Allow-Origin', '*');

      return response;
    }
    ```
  </Tab>
  <Tab value="Node.js">
    ```ts title="server.ts"
    createServer(async (req, res) => {
      // Handle preflight
      if (req.method === 'OPTIONS') {
        res.writeHead(204, {
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type',
        });
        res.end();
        return;
      }

      // Add CORS headers to response
      res.setHeader('Access-Control-Allow-Origin', '*');

      // ... your handler
    });
    ```
  </Tab>
</Tabs>

---

## Error Handling

```ts
export async function POST(req: Request) {
  try {
    const { messages } = await req.json();

    const result = await streamText({
      model: openai('gpt-4o'),
      messages,
    });

    return result.toTextStreamResponse();
  } catch (error) {
    console.error('Chat error:', error);

    return Response.json(
      { error: 'Failed to process chat request' },
      { status: 500 }
    );
  }
}
```

---

## Request Validation

Validate incoming requests with Zod:

```ts
import { z } from 'zod';

const ChatRequestSchema = z.object({
  messages: z.array(z.object({
    role: z.enum(['user', 'assistant', 'system']),
    content: z.string(),
  })),
});

export async function POST(req: Request) {
  const body = await req.json();

  const parsed = ChatRequestSchema.safeParse(body);
  if (!parsed.success) {
    return Response.json(
      { error: 'Invalid request', details: parsed.error.errors },
      { status: 400 }
    );
  }

  const { messages } = parsed.data;
  // ... continue with validated data
}
```

---

## Connecting Frontend

Point your frontend to your API endpoint:

```tsx title="app/providers.tsx"
'use client';

import { CopilotProvider } from '@yourgpt/copilot-sdk/react';

export function Providers({ children }: { children: React.ReactNode }) {
  return (
    <CopilotProvider runtimeUrl="/api/chat">
      {children}
    </CopilotProvider>
  );
}
```

For a separate backend server:

```tsx
<CopilotProvider runtimeUrl="http://localhost:3001/api/chat">
```

---

## Next Steps

- [LLM SDK](/docs/llm-sdk) - Core text generation functions
- [Backend Tools](/docs/tools/backend-tools) - Add server-side tools
- [Providers](/docs/providers) - Configure different LLM providers
