---
title: Server Setup
description: Configure your backend API for the Copilot SDK
icon: Server
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

Your server receives messages from the Copilot SDK frontend, calls the LLM, and returns the response.

---

## Quick Start

### 1. Install Dependencies

<Tabs items={['OpenAI', 'Anthropic', 'Google', 'xAI']}>
  <Tab value="OpenAI">
    ```bash
    npm install @yourgpt/llm-sdk openai
    ```

    <Callout>
    See [OpenAI Provider](/docs/providers/openai) for all available models.
    </Callout>
  </Tab>
  <Tab value="Anthropic">
    ```bash
    npm install @yourgpt/llm-sdk @anthropic-ai/sdk
    ```

    <Callout type="warn">
    Anthropic requires its own SDK (`@anthropic-ai/sdk`).
    See [Anthropic Provider](/docs/providers/anthropic) for models and extended thinking.
    </Callout>
  </Tab>
  <Tab value="Google">
    ```bash
    npm install @yourgpt/llm-sdk openai
    ```

    <Callout>
    Google uses OpenAI-compatible API.
    See [Google Provider](/docs/providers/google) for Gemini models.
    </Callout>
  </Tab>
  <Tab value="xAI">
    ```bash
    npm install @yourgpt/llm-sdk openai
    ```

    <Callout>
    xAI uses OpenAI-compatible API.
    See [xAI Provider](/docs/providers/xai) for Grok models.
    </Callout>
  </Tab>
</Tabs>

### 2. Set Environment Variables

```bash title=".env.local"
OPENAI_API_KEY=sk-...
# or ANTHROPIC_API_KEY=sk-ant-...
# or GOOGLE_API_KEY=...
# or XAI_API_KEY=...
```

### 3. Create Runtime

<Tabs items={['OpenAI', 'Anthropic', 'Google', 'xAI']}>
  <Tab value="OpenAI">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });
    ```
  </Tab>
  <Tab value="Anthropic">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createAnthropic } from '@yourgpt/llm-sdk/anthropic';

    const runtime = createRuntime({
      provider: createAnthropic({ apiKey: process.env.ANTHROPIC_API_KEY }),
      model: 'claude-sonnet-4-20250514',
      systemPrompt: 'You are a helpful assistant.',
    });
    ```
  </Tab>
  <Tab value="Google">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createGoogle } from '@yourgpt/llm-sdk/google';

    const runtime = createRuntime({
      provider: createGoogle({ apiKey: process.env.GOOGLE_API_KEY }),
      model: 'gemini-2.0-flash',
      systemPrompt: 'You are a helpful assistant.',
    });
    ```
  </Tab>
  <Tab value="xAI">
    ```ts
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createXAI } from '@yourgpt/llm-sdk/xai';

    const runtime = createRuntime({
      provider: createXAI({ apiKey: process.env.XAI_API_KEY }),
      model: 'grok-3-fast-beta',
      systemPrompt: 'You are a helpful assistant.',
    });
    ```
  </Tab>
</Tabs>

---

## Usage

<Tabs items={['Next.js / Hono', 'Express', 'Node.js']}>
  <Tab value="Next.js / Hono">
    ```ts title="app/api/chat/route.ts"
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });

    export async function POST(req: Request) {
      const body = await req.json();

      // Streaming - real-time response
      return runtime.stream(body).toResponse();

      // Non-streaming - wait for complete response
      // const result = await runtime.chat(body);
      // return Response.json(result);
    }
    ```
  </Tab>
  <Tab value="Express">
    ```ts title="server.ts"
    import express from 'express';
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const app = express();
    app.use(express.json());

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });

    // Copilot SDK - Streaming (SSE)
    app.post('/api/copilot/stream', async (req, res) => {
      await runtime.stream(req.body).pipeToResponse(res);
    });

    // Copilot SDK - Non-streaming (JSON)
    app.post('/api/copilot/chat', async (req, res) => {
      const result = await runtime.chat(req.body);
      res.json(result);
    });

    app.listen(3001);
    ```

    <Callout>
    See the full [Express Demo](https://github.com/yourgpt/copilot-sdk/tree/main/examples/express-demo) for all endpoint variations including raw streaming and text-only responses.
    </Callout>
  </Tab>
  <Tab value="Node.js">
    ```ts title="server.ts"
    import { createServer } from 'http';
    import { createRuntime } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
    });

    createServer(async (req, res) => {
      if (req.method !== 'POST') return;
      const body = JSON.parse(await getBody(req));

      if (req.url === '/api/copilot/stream') {
        // Copilot SDK - Streaming (SSE)
        await runtime.stream(body).pipeToResponse(res);
      } else if (req.url === '/api/copilot/chat') {
        // Copilot SDK - Non-streaming (JSON)
        const result = await runtime.chat(body);
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify(result));
      }
    }).listen(3001);

    function getBody(req: any): Promise<string> {
      return new Promise((resolve) => {
        let data = '';
        req.on('data', (chunk: any) => data += chunk);
        req.on('end', () => resolve(data));
      });
    }
    ```
  </Tab>
</Tabs>

| Method | Use Case | Returns |
|--------|----------|---------|
| `runtime.stream(body)` | Real-time chat, interactive UI | `StreamResult` with `.toResponse()`, `.pipeToResponse()` |
| `runtime.chat(body)` | Background tasks, batch processing | `{ text, messages, toolCalls }` |


---

## All Response Methods

The `runtime.stream()` method returns a `StreamResult` with multiple ways to consume the response:

### For Copilot SDK (SSE format)

| Method | Returns | Framework |
|--------|---------|-----------|
| `toResponse()` | Web `Response` (SSE) | Next.js, Hono, Deno |
| `pipeToResponse(res)` | Pipes SSE stream | Express, Node.js |

```ts
// Next.js / Hono
return runtime.stream(body).toResponse();

// Express
await runtime.stream(body).pipeToResponse(res);
```

### For Non-Streaming

| Method | Returns | Description |
|--------|---------|-------------|
| `collect()` | `CollectedResult` | Wait for full response |
| `text()` | `string` | Just get the final text |

```ts
// Get full result
const { text, messages, toolCalls } = await runtime.stream(body).collect();

// Or just the text
const text = await runtime.stream(body).text();

// Or use the convenience method
const result = await runtime.chat(body); // Same as stream().collect()
```

### For Direct Text Streaming (Not Copilot SDK)

<Callout type="warn">
These methods return plain text (`text/plain`) which the Copilot SDK **cannot parse**. Only use for direct streaming to non-Copilot clients.
</Callout>

| Method | Returns | Framework |
|--------|---------|-----------|
| `toTextResponse()` | Web `Response` (text/plain) | Next.js, Hono, Deno |
| `pipeTextToResponse(res)` | Pipes text stream | Express, Node.js |

### For Custom Handling

| Method | Returns | Description |
|--------|---------|-------------|
| `toReadableStream()` | `ReadableStream<Uint8Array>` | Raw stream for custom processing |

### Event Handlers

Process events as they stream (similar to Anthropic SDK):

```ts
const result = runtime.stream(body)
  .on('text', (text) => console.log('Text:', text))
  .on('toolCall', (call) => console.log('Tool:', call.name))
  .on('done', (result) => console.log('Done:', result.text))
  .on('error', (err) => console.error('Error:', err));

await result.pipeToResponse(res);
```

---

## Connect Frontend

Point your Copilot SDK frontend to your API endpoint:

```tsx title="app/providers.tsx"
'use client';

import { CopilotProvider } from '@yourgpt/copilot-sdk/react';

export function Providers({ children }: { children: React.ReactNode }) {
  return (
    <CopilotProvider runtimeUrl="/api/copilot/stream">
      {children}
    </CopilotProvider>
  );
}
```

For a separate backend server:

```tsx
// Streaming (default)
<CopilotProvider runtimeUrl="http://localhost:3001/api/copilot/stream">

// Non-streaming
<CopilotProvider
  runtimeUrl="http://localhost:3001/api/copilot/chat"
  streaming={false}
>
```

| Mode | Endpoint | CopilotProvider |
|------|----------|-----------------|
| Streaming (SSE) | `/api/copilot/stream` | `streaming={true}` (default) |
| Non-streaming (JSON) | `/api/copilot/chat` | `streaming={false}` |

---

## Advanced

<Accordions type="single">
  <Accordion title="Add Tools - Let the AI call functions on your server" id="tools">
    ```ts
    import { createRuntime, tool } from '@yourgpt/llm-sdk';
    import { createOpenAI } from '@yourgpt/llm-sdk/openai';
    import { z } from 'zod';

    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
      tools: [
        {
          name: 'getWeather',
          description: 'Get current weather for a city',
          inputSchema: {
            type: 'object',
            properties: {
              city: { type: 'string', description: 'City name' },
            },
            required: ['city'],
          },
          handler: async ({ city }) => {
            return { temperature: 72, condition: 'sunny' };
          },
        },
      ],
      agentLoop: {
        maxIterations: 10,
      },
    });
    ```
  </Accordion>

  <Accordion title="Add Persistence - Save conversations to your database" id="persistence">
    ```ts
    export async function POST(request: Request) {
      return runtime.handleRequest(request, {
        onFinish: async ({ messages, threadId }) => {
          await db.thread.upsert({
            where: { id: threadId },
            update: { messages, updatedAt: new Date() },
            create: { id: threadId, messages },
          });
        },
      });
    }
    ```
  </Accordion>

  <Accordion title="Track Token Usage - For billing and consumption limits" id="usage">
    Use the `onFinish` callback to track token usage for billing, credits, or consumption limits.
    Usage data is only available server-side and is not exposed to the client.

    **Option 1: With `handleRequest()` (Next.js)**
    ```ts
    export async function POST(request: Request) {
      return runtime.handleRequest(request, {
        onFinish: async ({ messages, usage }) => {
          if (usage) {
            await billing.recordUsage(userId, usage.totalTokens);
          }
        },
      });
    }
    ```

    **Option 2: With `stream()` (Express)**
    ```ts
    app.post('/api/chat', async (req, res) => {
      await runtime.stream(req.body, {
        onFinish: async ({ messages, usage }) => {
          if (usage) {
            console.log('Tokens:', usage.totalTokens);
            await billing.recordUsage(userId, usage.totalTokens);
          }
        },
      }).pipeToResponse(res);
    });
    ```

    **Option 3: With `chat()` (Non-streaming)**
    ```ts
    const result = await runtime.chat(body);
    // Usage is directly in the result for non-streaming
    console.log('Tokens:', result.usage?.totalTokens);
    ```

    | Field | Type | Description |
    |-------|------|-------------|
    | `usage.promptTokens` | `number` | Input tokens (prompt + context) |
    | `usage.completionTokens` | `number` | Output tokens (response) |
    | `usage.totalTokens` | `number` | Total tokens used |

    <Callout type="info">
    Usage is **never** sent to the client. Use `onFinish` for streaming, or access `result.usage` directly for non-streaming.
    </Callout>
  </Accordion>

  <Accordion title="Runtime Configuration - All options" id="config">
    ```ts
    const runtime = createRuntime({
      provider: createOpenAI({ apiKey: process.env.OPENAI_API_KEY }),
      model: 'gpt-4o',
      systemPrompt: 'You are a helpful assistant.',
      agentLoop: {
        maxIterations: 20,  // Max tool call cycles
        debug: true,        // Enable debug logging
      },
      tools: [/* server-side tools */],
      toolContext: {
        userId: 'user_123', // Passed to all tool handlers
      },
    });
    ```

    | Option | Type | Description |
    |--------|------|-------------|
    | `provider` | `AIProvider` | Provider instance |
    | `model` | `string` | Model ID (e.g., `'gpt-4o'`) |
    | `systemPrompt` | `string` | Default system prompt |
    | `agentLoop.maxIterations` | `number` | Max tool cycles (default: 20) |
    | `tools` | `ToolDefinition[]` | Server-side tools |
    | `toolContext` | `object` | Context passed to tool handlers |
  </Accordion>

  <Accordion title="CORS Configuration - For cross-origin requests" id="cors">
    ```ts title="app/api/chat/route.ts"
    export async function OPTIONS() {
      return new Response(null, {
        headers: {
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type',
        },
      });
    }

    export async function POST(req: Request) {
      const body = await req.json();
      const response = runtime.stream(body).toResponse();
      response.headers.set('Access-Control-Allow-Origin', '*');
      return response;
    }
    ```
  </Accordion>

  <Accordion title="Error Handling" id="errors">
    ```ts
    export async function POST(req: Request) {
      try {
        const body = await req.json();
        return runtime.stream(body).toResponse();
      } catch (error) {
        console.error('Chat error:', error);
        return Response.json({ error: 'Failed to process request' }, { status: 500 });
      }
    }
    ```
  </Accordion>
</Accordions>

---

## Direct AI Functions

For more control or standalone usage without the Runtime, you can use the AI functions directly:

| Function | Description | Link |
|----------|-------------|------|
| `streamText()` | Stream text in real-time | [Documentation](/docs/llm-sdk/stream-text) |
| `generateText()` | Generate complete text (non-streaming) | [Documentation](/docs/llm-sdk/generate-text) |

```ts
import { streamText, generateText } from '@yourgpt/llm-sdk';
import { openai } from '@yourgpt/llm-sdk/openai';

// Streaming
const stream = await streamText({
  model: openai('gpt-4o'),
  messages,
});
return stream.toDataStreamResponse();

// Non-streaming
const result = await generateText({
  model: openai('gpt-4o'),
  messages,
});
return Response.json({ text: result.text });
```

<Callout type="info">
**Note:** When using `streamText()` with Copilot SDK, use `toDataStreamResponse()` (not `toTextStreamResponse()`). See the [streamText documentation](/docs/llm-sdk/stream-text) for details.
</Callout>

---

## Next Steps

- [Tools](/docs/tools) - Learn more about frontend and backend tools
- [Providers](/docs/providers) - Provider-specific configuration
- [Chat History](/docs/chat-history) - Persist conversations across sessions
- [LLM SDK](/docs/llm-sdk) - Low-level AI functions
